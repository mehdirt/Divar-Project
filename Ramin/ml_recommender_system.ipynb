{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e8d942",
   "metadata": {},
   "source": [
    "## Part1: K-means with 10 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351906fb",
   "metadata": {},
   "source": [
    "### Load data and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6f1c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.cluster import KMeans\n",
    "from pyproj import CRS, Transformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import BinaryEncoder\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63658275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farzin\\AppData\\Local\\Temp\\ipykernel_23268\\4260200410.py:1: DtypeWarning: Columns (11,27,29,53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  divar_train = pd.read_csv('../Divar Dataset/Divar.csv')\n"
     ]
    }
   ],
   "source": [
    "divar_train = pd.read_csv('../Divar Dataset/Divar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b3364",
   "metadata": {},
   "source": [
    "### Part 1: `Kmeans with 10 clusters`<br>\n",
    "First we will try `K-means` on  4 different feature sets namely: `feature_set_[1,2,3,4]` to find a better clustering feature set.<br>\n",
    "To achieve the best set, we then evaluate our clustering model with the following `Evaluation Metrics`:<br>\n",
    "\n",
    "* `davies_bouldin_score`\n",
    "* `Inertia` (SSE - Sum of Squared Errors)\n",
    "\n",
    "Finally, we choose the `best feature set` to plot on Iran's map and show the clustering data comprehensively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259987b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "divar_train_cp = divar_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8fe4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for K-means\n",
    "feature_set_1 = divar_train_cp[['city_slug', 'price_value', 'building_size', 'cat3_slug']]\n",
    "feature_set_2 = divar_train_cp[['location_latitude', 'location_longitude', 'price_value']]\n",
    "feature_set_3 = divar_train_cp[['location_latitude', 'location_longitude', 'price_value', 'building_size', 'construction_year']]\n",
    "feature_set_4 = divar_train_cp[['location_latitude', 'location_longitude', 'price_value', 'rent_value', 'credit_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3279b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Feature Set 1\n",
    "# Handling missing values\n",
    "feature_set_1 = feature_set_1.dropna()\n",
    "\n",
    "# Encoding Nominal features\n",
    "# Frequency Encoding for city_slug\n",
    "freq_encoding = feature_set_1['city_slug'].value_counts().to_dict()\n",
    "feature_set_1['city_slug'] = feature_set_1['city_slug'].map(freq_encoding)\n",
    "\n",
    "# Binary Encoding for cat3_slug\n",
    "be = BinaryEncoder()\n",
    "temp = be.fit_transform(feature_set_1[['cat3_slug']])\n",
    "feature_set_1 = pd.concat([feature_set_1, temp], axis=1)\n",
    "feature_set_1.drop(columns=['cat3_slug'], inplace=True)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler_1 = StandardScaler()\n",
    "pf = PowerTransformer(method='yeo-johnson')\n",
    "feature_set_1['price_value'] = pf.fit_transform(feature_set_1[['price_value']])\n",
    "feature_set_1_scaled = scaler_1.fit_transform(feature_set_1)\n",
    "feature_set_1_scaled = pd.DataFrame(feature_set_1_scaled, columns=feature_set_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0379f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Feature Set 2\n",
    "# Handling missing values\n",
    "feature_set_2 = feature_set_2.dropna()\n",
    "\n",
    "# Feature Scaling\n",
    "pf = PowerTransformer(method='yeo-johnson')\n",
    "feature_set_2['price_value'] = pf.fit_transform(feature_set_2[['price_value']])\n",
    "scaler_2 = StandardScaler()\n",
    "feature_set_2_scaled = scaler_2.fit_transform(feature_set_2)\n",
    "feature_set_2_scaled = pd.DataFrame(feature_set_2_scaled, columns=feature_set_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d542e580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Feature Set 3\n",
    "# Handling missing values\n",
    "feature_set_3 = feature_set_3.dropna()\n",
    "\n",
    "# Digits conversion to english\n",
    "def persian_to_english(sample_input: str):\n",
    "    persian_digits = '۰۱۲۳۴۵۶۷۸۹'\n",
    "    english_digits = '0123456789'\n",
    "    trans_table = str.maketrans(persian_digits, english_digits)\n",
    "    return sample_input.translate(trans_table)\n",
    "\n",
    "feature_set_3['construction_year'] = feature_set_3.loc[:, 'construction_year'].apply(\n",
    "    lambda x: persian_to_english(x) if isinstance(x, str) else x)\n",
    "\n",
    "# change to only digits\n",
    "feature_set_3['construction_year'] = feature_set_3['construction_year'].replace('قبل از 1370', '1370')\n",
    "# change to numeric type\n",
    "feature_set_3['construction_year'] = pd.to_numeric(feature_set_3['construction_year'], errors='coerce')\n",
    "feature_set_3['age'] = 1404 - feature_set_3['construction_year']\n",
    "feature_set_3.drop(columns=['construction_year'], inplace=True)\n",
    "\n",
    "# Feature Scaling\n",
    "pf = PowerTransformer(method='yeo-johnson')\n",
    "feature_set_3['price_value'] = pf.fit_transform(feature_set_3[['price_value']])\n",
    "feature_set_3['building_size'] = pf.fit_transform(feature_set_3[['building_size']])\n",
    "scaler_3 = StandardScaler()\n",
    "feature_set_3_scaled = scaler_3.fit_transform(feature_set_3)\n",
    "feature_set_3_scaled = pd.DataFrame(feature_set_3_scaled, columns=feature_set_3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fedd77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing feature_set_4\n",
    "# fill price_value missing values with transformed_price comming form it's rent and credit\n",
    "def rent_to_price(rent, credit):\n",
    "    if rent < 0 or credit < 0:\n",
    "        return 0\n",
    "    credit_total = credit + (rent * 100) / 3\n",
    "    transformed_price = credit_total * 6\n",
    "    return max(0, transformed_price)\n",
    "\n",
    "# Handling missing values\n",
    "feature_set_4.loc[:, 'price_value'] = feature_set_4.apply(\n",
    "    lambda row: rent_to_price(row['rent_value'], row['credit_value']) if\n",
    "      (pd.isna(row['price_value']) & pd.notna(row['rent_value']) & pd.notna(row['credit_value'])) else row['price_value'],\n",
    "       axis=1)\n",
    "\n",
    "feature_set_4 = feature_set_4.drop(columns=['rent_value', 'credit_value'], axis=1)\n",
    "feature_set_4 = feature_set_4.dropna()\n",
    "\n",
    "# Feature Scaling\n",
    "feature_set_4_cp = feature_set_4.copy()\n",
    "pf = PowerTransformer(method='yeo-johnson')\n",
    "feature_set_4_cp['price_value'] = pf.fit_transform(feature_set_4_cp[['price_value']])\n",
    "scaler_4 = StandardScaler()\n",
    "feature_set_4_scaled = scaler_4.fit_transform(feature_set_4_cp)\n",
    "feature_set_4_scaled = pd.DataFrame(feature_set_4_scaled, columns=feature_set_4_cp.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57202b8",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "As we mentioned above, we use `kmean` with `n_cluster=10` for the feature sets. <br>\n",
    "Then we evaluate the trained models with <b>2 different metrics</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5605180e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               davies_bouldin        Inertia\n",
      "Feature Set 1        0.808282  637690.187921\n",
      "Feature Set 2        0.901254  224791.580539\n",
      "Feature Set 3        1.156761  585068.830427\n",
      "feature_set_4        0.784098  378977.759181\n"
     ]
    }
   ],
   "source": [
    "k_values = 10\n",
    "features_dict = {'Feature Set 1': feature_set_1_scaled,\n",
    "                 'Feature Set 2': feature_set_2_scaled,\n",
    "                 'Feature Set 3': feature_set_3_scaled,\n",
    "                 'feature_set_4': feature_set_4_scaled\n",
    "                 }\n",
    "results_dict = {}\n",
    "for feature_name, feature_data in features_dict.items():\n",
    "    kmeans = KMeans(n_clusters=k_values, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(feature_data)\n",
    "    davies_bouldin = davies_bouldin_score(feature_data, cluster_labels)\n",
    "    results_dict[feature_name] = {\n",
    "        'davies_bouldin': davies_bouldin,\n",
    "        'Inertia': kmeans.inertia_\n",
    "    }\n",
    "    \n",
    "print(pd.DataFrame(results_dict).T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0da06",
   "metadata": {},
   "source": [
    "#### Best Feature set: `feature_set_4`<br>\n",
    "As we can see from the results, `Davis Bouldin` is the lowest for this feature set. So we now, make it as out <b>best choice</b><br>\n",
    "Now we want to do `k-means` with `k=10` and with `feature_set_4`. But before that, we firts remove the data point whcih are not within `Iran's borders`. We use `geopandas` for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c06ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "del feature_set_1, feature_set_1_scaled, feature_set_2, feature_set_2_scaled\n",
    "del feature_set_3, feature_set_3_scaled, temp, freq_encoding, cluster_labels\n",
    "del feature_data, feature_name, features_dict, feature_set_4_cp, feature_set_4_scaled, kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02f22699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\ai\\Lib\\site-packages\\pyogrio\\core.py:35: RuntimeWarning: Could not detect GDAL data files.  Set GDAL_DATA environment variable to the correct path.\n",
      "  _init_gdal_data()\n"
     ]
    }
   ],
   "source": [
    "# We want to get only datapoints which are within Iran's borders\n",
    "# Build GeoDataFrame in WGS84\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    feature_set_4,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        feature_set_4[\"location_longitude\"],\n",
    "        feature_set_4[\"location_latitude\"]\n",
    "    ),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Load Iran border (Natural Earth)\n",
    "iran = gpd.read_file(\n",
    "    \"https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip\"\n",
    ")\n",
    "iran = iran[iran[\"NAME\"] == \"Iran\"].to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Spatial filter\n",
    "iran_geom = iran.geometry.iloc[0]\n",
    "gdf = gdf[gdf.within(iran_geom)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c3fb326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zones are:  [39 40 38 41]\n"
     ]
    }
   ],
   "source": [
    "# Add Iran's Zones\n",
    "gdf['utm_zone'] = (np.floor((gdf['location_longitude'] + 180) / 6) + 1).astype(int)\n",
    "print(\"Zones are: \", gdf['utm_zone'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "588abfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_latitude</th>\n",
       "      <th>location_longitude</th>\n",
       "      <th>price_value</th>\n",
       "      <th>geometry</th>\n",
       "      <th>utm_easting</th>\n",
       "      <th>utm_northing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.703865</td>\n",
       "      <td>51.373459</td>\n",
       "      <td>9.700000e+09</td>\n",
       "      <td>POINT (51.37346 35.70387)</td>\n",
       "      <td>533784.424602</td>\n",
       "      <td>3.951168e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>35.729832</td>\n",
       "      <td>51.505466</td>\n",
       "      <td>8.700000e+09</td>\n",
       "      <td>POINT (51.50547 35.72983)</td>\n",
       "      <td>545711.558185</td>\n",
       "      <td>3.954101e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35.712364</td>\n",
       "      <td>50.794781</td>\n",
       "      <td>6.500000e+08</td>\n",
       "      <td>POINT (50.79478 35.71236)</td>\n",
       "      <td>481437.130969</td>\n",
       "      <td>3.952066e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>35.778664</td>\n",
       "      <td>51.757549</td>\n",
       "      <td>2.600000e+09</td>\n",
       "      <td>POINT (51.75755 35.77866)</td>\n",
       "      <td>568467.028494</td>\n",
       "      <td>3.959664e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35.733952</td>\n",
       "      <td>51.380608</td>\n",
       "      <td>7.200000e+09</td>\n",
       "      <td>POINT (51.38061 35.73395)</td>\n",
       "      <td>534418.187237</td>\n",
       "      <td>3.954507e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    location_latitude  location_longitude   price_value  \\\n",
       "2           35.703865           51.373459  9.700000e+09   \n",
       "7           35.729832           51.505466  8.700000e+09   \n",
       "8           35.712364           50.794781  6.500000e+08   \n",
       "10          35.778664           51.757549  2.600000e+09   \n",
       "11          35.733952           51.380608  7.200000e+09   \n",
       "\n",
       "                     geometry    utm_easting  utm_northing  \n",
       "2   POINT (51.37346 35.70387)  533784.424602  3.951168e+06  \n",
       "7   POINT (51.50547 35.72983)  545711.558185  3.954101e+06  \n",
       "8   POINT (50.79478 35.71236)  481437.130969  3.952066e+06  \n",
       "10  POINT (51.75755 35.77866)  568467.028494  3.959664e+06  \n",
       "11  POINT (51.38061 35.73395)  534418.187237  3.954507e+06  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding UTM cordinations to the data points\n",
    "def apply_utm_conversion(group):\n",
    "    zone = group.name\n",
    "    wgs84_crs = CRS.from_epsg(4326)\n",
    "    utm_crs = CRS.from_epsg(32600 + zone)\n",
    "    transformer = Transformer.from_crs(wgs84_crs, utm_crs, always_xy=True)\n",
    "    \n",
    "    easting, northing = transformer.transform(\n",
    "        group['location_longitude'].values,\n",
    "        group['location_latitude'].values\n",
    "    )\n",
    "    \n",
    "    group['utm_easting'] = easting\n",
    "    group['utm_northing'] = northing\n",
    "    return group\n",
    "\n",
    "gdf = gdf.groupby(['utm_zone'], group_keys=False).apply(apply_utm_conversion, include_groups=False)\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a067129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "pf = PowerTransformer(method='yeo-johnson')\n",
    "gdf['price_value'] = pf.fit_transform(gdf[['price_value']])\n",
    "scaler_5 = StandardScaler()\n",
    "gdf_scaled = scaler_5.fit_transform(gdf[['location_latitude', 'location_longitude', 'price_value']])\n",
    "gdf_scaled = pd.DataFrame(gdf_scaled, columns=['location_latitude', 'location_longitude', 'price_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78c16276",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(gdf_scaled)\n",
    "gdf['cluster_number'] = cluster_labels\n",
    "\n",
    "centers_scaled = kmeans.cluster_centers_\n",
    "centers_original = scaler_5.inverse_transform(centers_scaled)\n",
    "\n",
    "# location_latitude, location_longitude\n",
    "centers_lat = centers_original[:, 0]  \n",
    "centers_lon = centers_original[:, 1] \n",
    "gdf['price_value'] = pf.inverse_transform(gdf[['price_value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ddc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Map\n",
    "fig = px.scatter_map(\n",
    "    gdf, \n",
    "    lat='location_latitude', \n",
    "    lon='location_longitude', \n",
    "    color='cluster_number',\n",
    "    hover_data=['price_value', 'utm_easting', 'utm_northing'],\n",
    "    map_style=\"open-street-map\",\n",
    "    zoom=10, \n",
    "    center={\"lat\": 32.4279, \"lon\": 53.6880},\n",
    "    title='Geographical Scatter Plot of samples with K-means Clusters (10 Clusters)',\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "# Add Cluster Centers\n",
    "fig.add_trace(go.Scattermap(\n",
    "    lat=centers_lat,\n",
    "    lon=centers_lon,\n",
    "    mode='markers+text',\n",
    "    marker=dict(size=20, color='red', symbol='star'),\n",
    "    text=[f'Cluster {i}' for i in range(len(centers_lat))],\n",
    "    textposition=\"top center\",\n",
    "    textfont=dict(size=12),\n",
    "    name='Cluster Centers',\n",
    "    hovertemplate='<b>Cluster %{text}</b><br>Lat: %{lat}<br>Lon: %{lon}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "queraAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
